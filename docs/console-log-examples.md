# コンソールログ出力例

このドキュメントでは、修正後のアプリケーションで表示されるコンソールログの例を示します。

## 上司の声登録時のコンソールログ

### 正常に動作している場合

```
🚀 1on1 発話比率測定アプリ（Azure版）を起動します...
📋 Azure Speech Service統合版を初期化中...
📄 DOMContentLoaded: ページの読み込みが完了しました
🔧 アプリケーションを初期化しています...
✅ Azure Speech SDKを確認しました Object { ... }
✅ マイク機能のサポートを確認しました
🎧 イベントリスナーを設定しています...
📑 タブ切り替え: Azure設定
...
✅ イベントリスナーの設定が完了しました
📖 Azure設定を読み込んでいます...
✅ 保存されたAzure設定を発見しました Object { region: "japaneast" }
✅ Azure Speech Configを初期化しました
🔍 登録状態を確認しています...
ℹ️ 上司の声が未登録です
✅ スクリプト（Azure版）の読み込みが完了しました

--- ユーザーが「上司の声登録」タブをクリック ---

📑 タブ切り替え: 上司の声登録
🔄 タブを切り替えます: registration
🔍 Azure設定を確認しています...
✅ Azure設定が設定されています

--- ユーザーが「声の登録を開始」ボタンをクリック ---

🎙️ 声の登録を開始します...
📌 [デバッグ] Azure Speech Service のダイアライゼーション機能を使用します
🔍 Azure設定を確認しています...
✅ Azure設定が設定されています
🔄 ConversationTranscriber を使用して上司の声を録音します...
📌 [デバッグ] これにより Azure Speech Service のダイアライゼーション機能が使用されます
✅ マイク入力を設定しました
✅ ConversationTranscriber を作成しました（ダイアライゼーション有効）
✅ 上司の声の録音を開始しました
📌 [ダイアライゼーション] Azure Speech Service が話者を自動識別します

--- ユーザーが話し始める ---

🗣️ [音声認識中] Object {
    speakerId: "Guest-1",
    認識テキスト: "こんにちは",
    状態: "認識中"
}

🗣️ [音声認識中] Object {
    speakerId: "Guest-1",
    認識テキスト: "こんにちは、これは",
    状態: "認識中"
}

✅ ========== 音声認識結果 ==========
📌 [結果 #1] Object {
    スピーカーID: "Guest-1",
    認識テキスト: "こんにちは、これはテストです。",
    発話時間: "2500ms",
    タイムスタンプ: "14:30:45"
}
📌 [ダイアライゼーション] Azure Speech Service が話者を識別しました
=====================================
👤 [上司の声を登録] スピーカーID: Guest-1

🗣️ [音声認識中] Object {
    speakerId: "Guest-1",
    認識テキスト: "マイクのテスト",
    状態: "認識中"
}

✅ ========== 音声認識結果 ==========
📌 [結果 #2] Object {
    スピーカーID: "Guest-1",
    認識テキスト: "マイクのテストをしています。",
    発話時間: "2100ms",
    タイムスタンプ: "14:30:48"
}
📌 [ダイアライゼーション] Azure Speech Service が話者を識別しました
=====================================

--- 10秒後、自動的に録音停止 ---

✅ 録音を停止しました
📊 [統計] 認識された発話数: 2
✅ 声の登録を完了します...
✅ ========== 上司の声の登録完了 ==========
📌 [登録情報] Object {
    保存されたスピーカーID: "Guest-1",
    登録日時: "2025/10/29 14:30:55",
    Azure_ダイアライゼーション使用: "はい"
}
=========================================
🔍 登録状態を確認しています...
✅ 上司の声が登録されています: Object {
    voiceProfileId: "Guest-1",
    profileDate: "1730204455000"
}
```

## 1on1測定時のコンソールログ

### 正常に動作している場合

```
--- ユーザーが「1on1測定」タブをクリック ---

📑 タブ切り替え: 1on1測定
🔄 タブを切り替えます: meeting
🔍 登録状態を確認しています...
✅ 上司の声が登録されています: Object { ... }

--- ユーザーが「1on1開始」ボタンをクリック ---

🎬 1on1測定を開始します...
📌 ========== Azure Speech Service 設定情報 ==========
📌 [確認] ConversationTranscriber を使用
📌 [確認] ダイアライゼーション機能: 有効
📌 [確認] 話者の自動識別: 有効
📌 [注意] 登録された音声プロファイルとの照合: 未実装
================================================
🔍 登録状態を確認しています...
✅ 上司の声が登録されています: Object { ... }
✅ ConversationTranscriberを作成しました
📌 [ダイアライゼーション] Azure Speech Service が会話中の話者を自動識別します
🎧 イベントハンドラーを設定しています...
📌 [ダイアライゼーション] Azure Speech Service の ConversationTranscriber を使用
✅ 会話の認識を開始しました
📌 [ダイアライゼーション] 話者の識別が開始されました

--- 上司が話す ---

🗣️ [1on1測定] 認識中: Object {
    スピーカーID: "Guest-1",
    認識テキスト: "今日の"
}

🗣️ [1on1測定] 認識中: Object {
    スピーカーID: "Guest-1",
    認識テキスト: "今日の進捗は"
}

✅ ========== 1on1測定 - 音声認識結果 ==========
📌 [認識結果] Object {
    スピーカーID: "Guest-1",
    認識テキスト: "今日の進捗はどうですか？",
    発話時間: "3200ms",
    タイムスタンプ: "14:35:10"
}
🔍 [話者識別処理] スピーカーID: Guest-1
📝 [話者識別] 最初の話者を上司として登録: Guest-1
📌 [ダイアライゼーション] Azure Speech Service が自動的に話者を識別しました
ℹ️ [注意] この実装では登録された音声プロファイルとの照合は行っていません
ℹ️ [注意] より高度な話者識別には Azure Speaker Recognition API の使用が必要です
📌 [話者識別結果] Object {
    スピーカーID: "Guest-1",
    識別結果: "上司",
    登録されたプロファイルID: "Guest-1"
}
============================================

--- 部下が話す ---

🗣️ [1on1測定] 認識中: Object {
    スピーカーID: "Guest-2",
    認識テキスト: "順調に"
}

✅ ========== 1on1測定 - 音声認識結果 ==========
📌 [認識結果] Object {
    スピーカーID: "Guest-2",
    認識テキスト: "順調に進んでいます。",
    発話時間: "2800ms",
    タイムスタンプ: "14:35:15"
}
🔍 [話者識別処理] スピーカーID: Guest-2
🎯 [話者識別結果] Object {
    スピーカーID: "Guest-2",
    判定: "部下",
    最初の話者_上司: "Guest-1",
    一致: "いいえ"
}
📌 [話者識別結果] Object {
    スピーカーID: "Guest-2",
    識別結果: "部下",
    登録されたプロファイルID: "Guest-1"
}
============================================

--- 会話が続く ---
...

--- ユーザーが「1on1終了」ボタンをクリック ---

⏹️ 1on1測定を停止します...
✅ 認識を停止しました
📊 測定結果を表示します...
📈 発話比率: Object {
    total: "30秒",
    manager: "12秒 (40.0%)",
    member: "18秒 (60.0%)"
}
📊 円グラフを描画します... Object { managerRatio: 40, memberRatio: 60 }
✅ 円グラフの描画が完了しました
💡 アドバイスを生成します... Object { managerRatio: 40 }
✅ アドバイスを表示しました: ✨ 素晴らしいバランスです！上司と部下が適切に対話できています。この調子で続けましょう。
✅ 1on1測定を停止しました Object {
    managerTime: "12秒",
    memberTime: "18秒"
}
⏹️ セッションが停止しました
```

## エラーが発生している場合

### パターン1: マイクの権限がない

```
🎙️ 声の登録を開始します...
📌 [デバッグ] Azure Speech Service のダイアライゼーション機能を使用します
...
✅ ConversationTranscriber を作成しました（ダイアライゼーション有効）
❌ 認識開始に失敗しました: NotAllowedError: Permission denied
```

**解決方法:** ブラウザの設定でマイクの使用を許可してください。

### パターン2: Azure設定が間違っている

```
🎙️ 声の登録を開始します...
📌 [デバッグ] Azure Speech Service のダイアライゼーション機能を使用します
...
❌ 認識開始に失敗しました: Error: Invalid subscription key or region
```

**解決方法:** Azure設定タブでサブスクリプションキーとリージョンを確認してください。

### パターン3: スピーカーIDが取得できない

```
🗣️ [音声認識中] Object {
    speakerId: "Unknown",
    認識テキスト: "テストです",
    状態: "認識中"
}

✅ ========== 音声認識結果 ==========
📌 [結果 #1] Object {
    スピーカーID: "Unknown",
    認識テキスト: "テストです。",
    発話時間: "1200ms",
    タイムスタンプ: "14:30:45"
}
📌 [ダイアライゼーション] Azure Speech Service が話者を識別しました
=====================================
```

**原因:**
- 発話が短すぎる（3秒未満）
- 環境ノイズが多い
- リージョンがダイアライゼーションに対応していない

**解決方法:**
- もっと長く話す（3秒以上）
- 静かな環境で実行
- ダイアライゼーションに対応しているリージョンに変更

**ダイアライゼーション対応リージョン（2024年時点）:**
- `japaneast` (東日本) - 推奨
- `eastus` (米国東部)
- `westus2` (米国西部2)
- `westeurope` (西ヨーロッパ)
- `southeastasia` (東南アジア)

※最新の対応リージョンは [Azure公式ドキュメント](https://learn.microsoft.com/ja-jp/azure/cognitive-services/speech-service/regions) を参照してください。

## ログの見方ガイド

### 重要なログの意味

| ログ | 意味 | 状態 |
| ---- | ---- | ---- |
| `✅ ConversationTranscriber を作成しました` | Azure Speech Service が正しく初期化された | ✅ 正常 |
| `📌 [ダイアライゼーション] Azure Speech Service が話者を識別しました` | ダイアライゼーションが動作している | ✅ 正常 |
| `スピーカーID: "Guest-1"` | 話者が識別された | ✅ 正常 |
| `スピーカーID: "Unknown"` | 話者が識別できなかった | ⚠️ 注意 |
| `❌ 認識開始に失敗しました` | エラーが発生した | ❌ エラー |

### デバッグのポイント

1. **音声認識が動作しているか確認**
   - `🗣️ [音声認識中]` が表示される → ✅ OK
   - 何も表示されない → ❌ マイクの問題

2. **ダイアライゼーションが動作しているか確認**
   - `スピーカーID: "Guest-1"` など → ✅ OK
   - `スピーカーID: "Unknown"` → ⚠️ 環境を改善

3. **エラーメッセージを確認**
   - `❌` で始まるメッセージを探す
   - エラー内容に応じて対処

## 現在の実装の制限事項と注意点

### 実装されている機能 ✅

1. **Azure Speech Service の ConversationTranscriber**
   - リアルタイム音声認識
   - 自動話者識別（ダイアライゼーション）
   - スピーカーIDの取得
   - 日本語音声認識

2. **詳細なログ出力**
   - 音声認識結果の表示
   - スピーカーIDの表示
   - 話者識別結果の表示

### 未実装の機能 ⚠️

1. **音声プロファイルとの照合**
   - 現在は実装されていません
   - スピーカーIDの順番で判定しています
   - ログに `ℹ️ [注意] この実装では登録された音声プロファイルとの照合は行っていません` と表示されます

2. **Azure Speaker Recognition API の使用**
   - より高度な話者識別には別のAPI（Speaker Recognition API）が必要
   - ブラウザJavaScript SDKでは音声プロファイル作成が未サポート
   - REST API の直接呼び出しが必要

### 話者識別の動作方式

**現在の方式（簡易実装）:**
- 最初に話した人を「上司」として識別
- 2番目以降の話者を「部下」として識別
- スピーカーIDの一致で判定

**制約:**
- 1on1開始時は上司から話し始める必要がある
- 話者が2人を想定（3人以上は未対応）
- 音声の特徴量（声質、話し方）での識別は行っていない

**将来の改善予定:**
- Azure Speaker Recognition API との統合
- 登録された音声プロファイルとの照合
- より高精度な話者識別

---

**作成日:** 2025年10月29日  
**対象バージョン:** Azure Speech Service版 1.0
